
%% bare_jrnl.tex
%% V1.4a
%% 2014/09/17
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8a or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_conf_compsoc.tex,
%%                    bare_jrnl_compsoc.tex, bare_jrnl_transmag.tex
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices and paper sizes can       ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/tex-archive/info/epslatex/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
\ifCLASSOPTIONcompsoc
  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\else
  \usepackage[caption=false,font=footnotesize]{subfig}
\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/




% *** FLOAT PACKAGES ***
%
% \usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/


% \usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Do not use the stfloats baselinefloat ability as IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
\usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/dblfloatfix/




\ifCLASSOPTIONcaptionsoff
  \usepackage[nomarkers]{endfloat}
 \let\MYoriglatexcaption\caption
 \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/endfloat/
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/url/
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

% My packages
\usepackage{comment}
\usepackage{algorithm}
\usepackage{lipsum}
\usepackage{rumathmacros}

% My mods:
\newcommand{\papertitle}{Constructing the Pareto Front using Limited Information: A Case of the Opposition based Solution Generation Scheme}
\graphicspath{%
	{./results/zdt1/}%
	{./results/zdt2/}%
	{./results/zdt3/}%
	{./results/zdt4/}%
	{./results/zdt6/}%
	{./results/dtlz1/}%
	{./results/dtlz2/}%
	{./results/dtlz3/}%
	{./results/dtlz4/}%
	{./results/dtlz5/}%
	{./results/dtlz6/}%
	{./results/dtlz7/}%
	{./figs/}%
}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
% \title{Bare Demo of IEEEtran.cls for Journals}
\title{\papertitle}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

% author{AKM~Khaled~Ahsan~Talukder,\\\textbf{Experiment Report for CSE 890 Course}}
\author{Author~1,~\IEEEmembership{Member,~IEEE,}
         Author~2,~\IEEEmembership{Member,~OSA,}
         and~Author~3,~\IEEEmembership{Member,~IEEE}% <-this % stops a space
% \thanks{M. Shell is with the Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, 30332 USA e-mail: (see http://www.michaelshell.org/contact.html).}% <-this % stops a space
\thanks{Author-1 and Authtor-2 are with Anonymous University.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2005; revised September 17, 2014.}%
}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~13, No.~9, September~2014}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2014 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
In this paper we investigate a curious example of opposition based solution generation applied to an evolutionary multi-objective optimization (EMO) algorithm, namely on NSGA-II. In this paper we will see how the concept of Opposition Based Learning (OBL) can be reformulated to suite the Multi-objective Optimization (MOP) settings. We have devised a simple and an intuitive approach for OBL so that it can be applied to any elitist EMO algorithm. We have also conducted an in-depth analysis of the efficacy of our approach to a wide range of benchmark MOP test functions. We have also proposed a definitive guideline to how to find the extreme solutions on the true Pareto-front. Our proposed model utilizes a deterministic solution generation scheme that is guided by the opposite traits imposed on the current population individuals. Our study have also addressed some boundary issues -- such as the quantitative applicability of the opposite solutions generated by our approach.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
% \begin{IEEEkeywords}
% IEEEtran, journal, \LaTeX, paper, template.
% \end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
% \IEEEPARstart{T}{his} demo file is intended to serve as a ``starter file''
% for IEEE journal papers produced under \LaTeX\ using
% IEEEtran.cls version 1.8a and later.

\IEEEPARstart{I}{n} the recent years, the idea of Opposition Based Learning (OBL) has been enjoying a noticeable attention among the AI and OR practitioners. The idea of OBL is to accelerate the learning rate (or convergence rate) by imposing an opposite estimate of the current solution, and deliberately introducing them to influence the search trajectory. This idea was first introduced in \cite{obl-main} and has been demonstrated its effectiveness in different scenarios -- from Reinforcement Learning (RL) \cite{obl-rl}, Differential Evolution (DE) \cite{ode-main} to robotics \cite{ode-robot}. 

In all of the cases, the OBL comes into play specially during the initialization phase of the learning algorithm (or optimization algorithm), the argument behind this strategy is that a completely arbitrary (i.e. blind) initialization is no better than an informed boot-strap. Since a random sampling or selection of solutions from a given population has the possibility of visiting or even revisiting unpromising regions of the search space. In \cite{ode-main}, it has also been demonstrated that the chance of such revisits are lower in the case of opposite than it is for purely random initialization. There is also a formal proof that shows that, in general, the opposite estimations are more likely to be closer to the optimal solution than the completely arbitrary ones \cite{theory-1}. A more details of a formal probabilistic analysis of opposition based learning can also be found in \cite{theory-2}.

Moreover, especially for the case of optimization algorithms, it has been demonstrated that keeping a small ratio of solutions with the opposite estimate helps to converge better \cite{opbil} -- and most of the recent studies are inspired by this approach. For example, in \cite{omoead} similar strategy has been used in the case of MOEA/D \cite{moead-main}. We have also seen identical examples in \cite{omode}. Another relevant study can be found in \cite{opso}, where the opposition based initialization has been tested with the Particle Swarm Optimization (PSO) scenario to induce a better convergence. Given all of these studies, still we could not find a good example of this idea applied to the Evolutionary Multi-objective Optimization (EMO) cases -- more importantly -- in a more meaningful and precise way. In this study we will try to fill this gap in an interesting and in a simpler way. 

In our approach, we will address this idea of \textit{opposition} in a different perspective, we will classify the (not to be confused with the term ``classification'' in machine learning domain) solutions with respect to some \textit{desired traits} that we will like to have. Then we will use this information to deterministically generate new solutions in a most viable locations in the search space -- and this operation will be dictated by our special notion of opposition. In fact, our approach is somewhat similar in spirit to the previous studies done in \cite{sts-1}, \cite{sts-2} and \cite{directional-mutation}. Except the fact that our approach is extremely simple and does not assume any special property on the underlying search space, moreover, our approach does not build a computationally expensive models as done in \cite{search-history}\cite{segment-search}.

This paper is organized as follows -- first we will discuss why this idea of OBL needs to be reinterpreted for the Multi-objective Optimization (MOP) setting. Next we discuss one interesting limitation that most EMO algorithms suffer from -- the \textit{search trajectory bias}, and also give some argument on why such hindrance becomes inevitable. Then we will discuss how the idea of OBL can come into rescue. In our model, we generate the opposite points in a strictly deterministic way, by carrying out the arbitration of \textit{opposition} in a different (and a more MOP relevant manner). To do this, we utilize the extreme solutions on the true Pareto-front (PF), and the next section discusses how to find them efficiently. After that we will describe our main algorithm in details. Then we will show our experiments with different benchmark MOP problem sets. 

We will also see some boundary issues with our models -- like how the opposite points get utilized during the run, and some what-if analysis by integrating our models into the canonical EMO algorithm, namely in NSGA-II \cite{nsga2-main}. We will also see how this approach becomes more useful if the underlying search space becomes more complex or computationally more expensive to explore. Then we conclude our study by discussing some future guidelines to extend our idea.

% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)
% I wish you the best of success.

% \hfill mds
 
% \hfill September 17, 2014

\section{An Alternative Interpretation of Opposition}
\label{sec:alternative-interpretation}
As we have already discussed in the previous section, in most of the studies, the idea of \textit{opposition} is employed as an incorporation of new solutions with a certain kind of \textit{opposite traits} into the existing population. Such traits could be interpreted in terms of different problem domain perspectives. For example, the \textit{opposite} solutions could be -- i) the ones with complete opposite representation from the current best individual, ii) the ones with the opposite estimates from the other spectrum of the variable bounds (i.e. in the case of real valued optimization). However, the injection of the opposite solutions could cause a re-route from the continuing search trajectory and thus could be a misleading operation -- in a sense that the opposite solutions might be useful given that the search space follows a desired pattern. 

For example, if the task is to solve the N-queen problem, then the opposite representation of the current best can result into another valid global optima. One can easily verify this by computing the reverse assignment of queens from a existing optimal solution; and it will eventually take us to another global optima. This observation also assumes that the search space is multi-modal, following from the fact that the reverse representation of the current best solution needs to be an optimum for another peak of the search space. Therefore, most of the standard opposition based algorithms inject the opposite solutions during the optimization start-up; or maintain a constant (generally low) ratio of opposite points throughout the run. Therefore, the standard opposite injection scheme could only be effective given the two assumptions on the underlying search space are valid -- multi-modality and the solution symmetry. For this reason, what we think -- such approach is quite unwieldy to directly incorporate into a large scale numerical optimization problems, e.g. multi-objective optimization problems (MOPs).

Moreover, we have also seen some examples of opposition based algorithm for Q-learning/TD-learning like scenarios \cite{obl-rl}. Similar argument can be made, as the reinforcement-learning algorithms are inherently greedy (as they rely on the Bellman's optimality principle). And the opposite actions during the learning phase introduces a noise; so that the search can branch out to alternative choices. In that sense, we can say that the injection of opposite solutions can be considered as a different form of \textit{variation operator} in population based stochastic search. 

However, in the case of complex problem solving, the existing opposition based solution generation schemes may not be always effective. As we think, they are ``blind'' -- the opposite solutions are generated from the current best in an arbitrary manner. The existing approaches do not take account other important requirements that a good solution needs to be abide by. For example, if we arbitrarily create an completely opposite solution from the current best without considering any constraint, we could end up with a solution with a worst fitness value or a solution that violates constraints. Therefore, we think this idea of opposition needs to be revised, so that we can model the opposite traits as the choice between the \textit{good} and \textit{bad} solutions.

Moreover, in the numerical optimization scenarios, we hardly have rooms to make any assumption about the multi-modality of the search space and/or the symmetry of the solution representation (i.e. unlike such assumptions could be made about the N-queen problem). Therefore, in this paper, we have revised the notion of opposition in terms of the preference criteria imposed on a solution. For example, most evolutionary multi-objective optimization (EMO) algorithms aim to maximize two principal properties -- i) the convergence and ii) the diversity, since the quality of a MOP solution depends on these two factors. Therefore, let us re-consider the opposite point generation/injection in a different perspective --
\begin{itemize}
	\item Opposite Convergence: A solution \textit{far} from the true Pareto-front is \textit{opposite} to any solution that is \textit{closer} to the true Pareto-front.
	\item Opposite Diversity: An \textit{isolated} solution on the true Pareto-front is \textit{opposite} to a \textit{crowded} solution. 
\end{itemize}

By taking the above two principles into account, we will deterministically generate opposite solutions during the search. Obviously, the deterministic point generation scheme will only consider an opposite trait that is \textit{good}. In the next section we will see, how the existing EMO algorithms shows the limitations in maintaining this two opposite traits during the search (i.e. solution generation) process.  

% Figure
\begin{figure}[!pb]
	\centering
	\includegraphics[width=0.5\textwidth]{zdt4-bias}
	\caption{The effect of the \textit{search trajectory bias} could be seen when we try to solve ZDT4 problem with NSGA-II. Here we can see a long streak of crowded solution near the objective \(f_2\), where the distribution of solutions near the objective \(f_1\) is extremely sparse.}
	\label{fig:zdt4-unbalanced-snapshot}
\end{figure}
%
\section{Limitations with the Canonical EMO Algorithms: The Search Trajectory Bias}
\label{sec:limitation-canonical}
Most of the standard EMO algorithms (e.g. NSGA-II, SPEA-II \cite{spea2-main} etc.), are elitist by design. They are also ``opportunistic'' in a sense that the population always try to converge to a particular portion of the Pareto-front (PF) which seems to be easier to solve at that moment. They also show preferences over a certain objective function which needs less exploration than the other. We can see such bias in the search when we try to solve the ZDT4 problem using NSGA-II. In this case, the first objective is easier to optimize than the second one, the readers can verify this fact from the Figure \ref{fig:zdt4-unbalanced-snapshot}. Therefore, the search trajectory deliberately accumulates more points over the first objective to optimize one particular portion of the Pareto-front. Moreover, while putting more solutions to the vicinity of one particular objective axis, the search trajectory looses the uniformity by forming a crowded streak of points along that axis; on the other hand we can see that there is almost no solution on the other spectrum of the objective space. This kind of non-symmetric search behaviour is, we think, causes a hindrance to the optimization procedure. Therefore, it would be helpful if we could selectively inject points during the search where the solution distribution is more sparse. In addition, we also think that this biased nature of the search trajectory could degrade with the addition of more objective functions. Moreover, this could also lead to a stagnation on the local optima, given that the search space has a lot many of them.

Given this specific scenario, now the main problem is to devise a way to deterministically generate points where the distribution of the solution is sparse. Here we assume that the lesser the number of solutions in a vicinity of objective \(f_i\), the harder it is to solve. This would be easy if we know the exact mapping of the design variable to objective values -- however such mapping is always unavailable and above all it is very expensive to infer. Another way could be to mutate the points where the solution is more sparse, but we think as the original algorithm already goes through such step, it is not going to be very effective\footnote{However, in the section \ref{subsec:nsga2re} we will demonstrate this fact that just mutating the sparse solutions does not help much.}. 

In this paper we are going to demonstrate a very effective approach to address this issue, we will show how we can maintain a balanced distribution of solutions that is parallel to the true PF. The proposed approach is also extremely effective in fast converging to the true PF as well.   

\section{The Deterministic Opposite Point Generation Scheme}
\label{sec:generation-scheme}
As we have discussed in the section \ref{sec:alternative-interpretation}, we will utilize the so-called notion of \textit{opposition} to deterministically generate points into strategically useful places on the search space. In principle, we do not assume any exact mapping over the design variables to objective values, and we apply a linear translation to achieve our goal effectively\footnote{As a matter of fact, we will also see in the later sections that such opposite points are generally \(\sim 30\%\) useful in most cases and they are more effective during the initial generations -- which is also a very interesting finding.}. In essence, instead of considering \textit{sparsity} and \textit{crowd} as two opposite traits, our method can impose any criteria like \textit{being `x'} and \textit{being `y'} as two opposites. Being said that, linear translation is a simplest way to deterministically generate \textit{`x' like} solutions from \textit{`y' like} ones. 

So, the initial step is to infer the true PF before starting the actual optimization run. To do this, we depend on barely \(k\) number (\(k = \text{number of objectives}\)) of extreme (or near extreme) points on the true PF since we can safely assume that the population will eventually reach to the vicinity of those extreme points in the end. Moreover, the extreme points will be used as a pivot to arbitrate between the opposite traits over the existing solutions. Also note that we are not going to deterministically define which portion of the search space is less easy (or hard) and so forth -- we will try to devise a technique that will automatically address and solve such issues on the fly.

Another reason to fixate over the \(k\) extreme points is that we also wanted to keep the algorithm simple so that it can only utilize the ``minimal information'' of the true PF. We also think it's valid to assume that any PF could be bounded by at least \(k\)-extreme points for any \(k\)-objective problem. Although, if we could supply other intermediary points on the true PF, we will be able to see a better performance gain with the existing model, however supply of \(1\) extra true PF solution comes with an added cost of extra function evaluations. As the extreme (or near extreme) points are the pivot to define the notion of \textit{opposite} in our case, we will start the next section by discussing how to find them efficiently.

\subsection{Finding the Extreme Points}
\label{sec:find-extreme-points}
Extreme points on the Pareto-front could be found using global search as well \cite{nadir-estimation}, however our goal was to save the extra computational cost as much as possible\footnote{The readers might be aware that efficiently finding the extreme points on the true Pareto-front is itself a separate research problem.}. Therefore, we resort to the classical single-objective optimization methods to solve this problem. Our choice of such algorithms were limited to, namely, the Interior Point (IP) method and the Mesh Adaptive Direct Search (MADS)\footnote{We have used the \texttt{fmincon()} and the \texttt{patternsearch()} routine in MATLAB (v. R2014a) for IP method and MADS respectively.}. As we also did not want to spend the valuable function evaluations for this purpose, we have conducted this extreme solution search as a fixed budget operation. Depending on the difficulty of the problems, appropriate routine parameters were empirically found out and they are problem dependent. These settings can be summarized as follows: 
%
\begin{itemize}
	\item If the problem has no local optima then we use IP method (i.e. \texttt{fmincon()}), it has been found to be comparatively less expensive even if the variable size is large.
	\item If the problem has local optima, MADS (i.e. \texttt{patternsearch()}) is faster for finding more accurate extreme points. However, if the number of objective is $k > 2$, then these settings are found to be more useful:
		\begin{itemize}
			\item \texttt{InitialMeshSize:} \(1/\text{population size}\)
			\item \texttt{Search Method:} \texttt{@MADSPositiveBasis2N} to start searching with $2N$ random directions, where $N =$ number of variables.
			\item and keeping \texttt{CompletePoll:} to \texttt{on} and\\ \texttt{CompleteSearch:} to \texttt{on}
		\end{itemize}
\end{itemize}
%
The readers should be aware that the algorithm parameter that we have found is not a general setting, they have been set according to the problem structure, so they are subjected to empirical investigations and the problem domain-knowledge. 

The actual extreme point computation algorithm was conducted in two steps -- given a particular objective function \(f_i\), first we try to solve it directly using either IPM or MADS (depending on the problem type); then after some \(\frac{T}{2}\) iterations once we find a reference solution \(\mathbf{z}\) that is hopefully close to \(f_i\)'s optima, then we construct a so-called Augmented Achievement Scalarizing Function (AASF) \cite{asf} from \(f_i\) as \(f_{\text{aasf}} = \max_{j=1}^k w_j(f_j(\mathbf{x}) - z_j) + \rho \sum_{j=1}^k w_j(f_j(\mathbf{x}) - z_j)\) and solve it again for \(\frac{T}{2}\) iterations. Here, we set \(w_i = 0.9\), \(w_{j \neq i} = \frac{1}{10(k-1)}\) and \(\rho = 0.0001\). 

To limit the function evaluations, we kept \(T\) to a constant value (as a budget). For all problems, we have fixed this maximum iteration count to the \(\frac{1}{4}\)-th of the total generation specified. More precisely, $T = \frac{1}{k}(\frac{1}{4}N_p N_{\text{gen}})$, where $k = \text{no. of objectives}$, $N_p = \text{population size}$ and $N_{\text{gen}} = \text{maximum generation}$. A basic listing for this routine is presented in Algorithm \ref{algo:find-extreme-points}. The set of the extreme points \(E^\ast\) generated from this algorithm may not contain all the unique solution, and also they might not be the true extreme always, they can be weakly dominated solutions by the true PF extremes. However, our approach can utilize them efficiently to converge to the true PF extremes.
%
\begin{algorithm}[!tp]
\caption{Find Extreme Points}
\label{algo:find-extreme-points}
\begin{algorithmic}[1]
	\STATE $k \leftarrow$ no. of objectives
	\STATE $N_p \leftarrow$ population size
	\STATE $N_{\text{gen}} \leftarrow$ maximum generation
	\STATE $T \leftarrow \frac{1}{k}(\frac{1}{4}N_p N_{\text{gen}})$
	\STATE $E^\ast \leftarrow \emptyset$, an empty solution set
	\FOR{$i$ from $1$ \TO $k$}
		\STATE $f_i \leftarrow$ $i$-th objective function
		\STATE $x_i \leftarrow $ random initial vector
		\REPEAT
			\STATE $x_i \leftarrow$ solve $f_i$ with IP method (or MADS) 
		\UNTIL{$\frac{T}{2}$ function evaluation reached}
		\STATE $f_{\text{aasf}} \leftarrow $ construct AASF function from $f_i$
		\REPEAT
			\STATE $x_i \leftarrow$ solve $f_{\text{aasf}}$ with  IP method (or MADS)
		\UNTIL{$\frac{T}{2}$ function evaluation reached}
		\STATE $E^\ast \leftarrow \{E^\ast \cup x_i\}$
	\ENDFOR
	\RETURN $E^\ast$
\end{algorithmic}
\end{algorithm}
%
\subsection{The Opposite Solution Generation Algorithm}
\label{sec:generate-pivot-points}
Once the extreme points are discovered, now we utilize them to generate the so-called \textit{opposite} points during the main evolutionary runs. On each generation, we select 25\% of the best individuals (front-wise) from the current population and deterministically change them to generate opposite solutions -- in such a way that we can address the strategically preferable places. And to conduct this variation, we will utilize the points in the set \(E^\ast\) as pivot points. We call these points as ``pivot'' since we will selectively try to generate points around these pivots. However, before doing this, we will \textit{refine} our pivot points \(E^\ast\) in a certain way. 

The \textit{refinement} starts by finding the current population extreme points \(E_c\) and merging them with the set \(E^\ast\) such that \(E = \{E_c \cup E^\ast\}\). Next we apply the non-dominated sort on \(E\) to find the Pareto-front within this set. We apply this sorting because we are not still sure if \(E^\ast\) contains true PF extremes. This sorting will keep the true extreme points if ones are found in the later generations. After this step, we select the points from \(E\) that are on the best front and with \(\infty\) crowding distances\footnote{By ``crowding distance'', we mean the inter-solution distances computed in NSGA-II.}. Lets denote these selected points as \(E'\). Now at this point, two situations are possible:
%
\begin{itemize}
	\item The set \(E'\) contains only the solutions \(E^\ast\) while we are in the initial generations, or
	\item The set \(E'\) contains the solutions \(E_c\) while we are in the later phase of the generations, where \(E_c\) are the true PF extreme. 
\end{itemize}
%
However, during the intermediate generations, it can also happen that we may include some solutions into \(E\) that weakly dominate a subset of points already in \(E\), this inclusion will reduce the expected spread of the pivot points -- that may diminish the effect of maintaining the diversity. For example, if the actual true PF is a broken Pareto-front, and if \(E\) contains the extreme points from one broken edge, then we need to expand the current edges so that the refinement procedure can include points from the further extreme ends. Therefore, if there exist a point in \(E - E'\) that is on the best front and also weakly dominated by any point in \(E'\), then we replace the weakly dominating point from \(E'\) with the one from the set \(E - E'\). The readers might have already noticed that \(|E'| \le |E|\).
%
\begin{algorithm}[bp]
\caption{Generate Pivot Points}
\label{algo:generate-pivot-points}
\begin{algorithmic}[1]
	\REQUIRE true PF extreme points $E^{\ast}$ from Algorithm \ref{algo:find-extreme-points}
	\STATE $E_c \leftarrow$ the extreme points from the current PF
	\STATE $E \leftarrow \{E^\ast \cup E_c\}$
	\STATE rank points in $E$, $E \rightarrow \{\mathcal{F}_1, \mathcal{F}_2, \ldots, \mathcal{F}_n\}$
	\STATE take the best front in $E'$, $E' \leftarrow \mathcal{F}_1$
	\FOR{all points $p_i$ in $E - E'$}
		\IF{$p_i$ weakly dominates any $p_j \in E'$}
			\STATE replace $p_j$ by $p_i$
		\ENDIF
	\ENDFOR
	\STATE update $E^\ast$, $E^\ast \leftarrow E'$
	\STATE $G \leftarrow$ find $k$ intermediary gap points from the current PF
	\STATE $E' \leftarrow \{E' \cup G\}$
	\RETURN $E'$
\end{algorithmic}
\end{algorithm}
%

% Figure
\begin{figure*}[!hbp]
	\centering
	\includegraphics[width=0.9\textwidth]{point-generation}
	\caption{The illustration of lines 9--12 in Algorithm \ref{algo:onsga2}. The right axes are the variable space and the left axes are the corresponding objective space. The point \(\mathbf{x_c}\) is the child (black circle) and \(\mathbf{x_p}\) is the parent (white circle). The point \(\mathbf{v}\) are the pivot points (grey circles). The operation will choose one of the directions denoted by \(L_1\), \(L_2\) or \(L_3\). If \(\mathbf{x_c}\) violates the variable bound then it is reverted back to the vicinity of the corresponding pivot point \(\mathbf{v}\).}
	\label{fig:opposite-creation}
\end{figure*}
%
Now, at this point, we can ensure that the set \(E'\) contains either true PF extremes or points near them. Now if we can generate new points near \(E'\), they will induce both better convergence and diversity. In section \ref{sec:limitation-canonical}, we have discussed a scenario where we can see how the bias in the search trajectory is introduced. However, the difference in the relative difficulty of the objective functions may not be the only reason for a bias. The imbalance in the solution distribution could happen for other reasons as well. For example, a disconnected Pareto-front, a local optimal front or a specific portion of the Pareto-front being more difficult to solve than the rest. In such cases, we can see a \textit{gap} forming over the Pareto-front during the search, we can see such a convergence pattern in many problems. As an example, we can see similar effect in solving ZDT4 problem as illustrated in Figure \ref{fig:zdt4-gap-snapshot}. To address these \textit{gaps}, we also find the solutions with \(k\)-highest (\(k = \) no. of objectives) crowding distance from the best front that are not \(\infty\), and call them as set \(G\). Clearly, the \(G\) solutions are those that reside on the edge of the broken front. Now we add the \(G\) to the set \(E'\), thus we make \(E'\) as the final ``pivot'' solutions to generate the opposite points. This should be also noted that \(|E'| > k\). This procedure is presented in Algorithm \ref{algo:generate-pivot-points}.
% Figure
\begin{figure}[tp]
	\centering
	\includegraphics[width=0.5\textwidth]{zdt4-gap}
	\caption{The effect of the \textit{gap} in the trade-off, could be seen when we try to solve ZDT4 problem with NSGA-II.}
	\label{fig:zdt4-gap-snapshot}
\end{figure}
%
%
\subsection{Integrating into an Elitist EMO Algorithm: NSGA-II}
\label{sec:onsga2r}
As we have mentioned at the beginning that we select front-wise best 25\% of the current population for opposite point generation. We go through each of them randomly and every time we pick \(k\) number of random points from \(E'\) and pick the pivot point that is the furthest from it, and find the opposite vector using a linear translation. A straight-forward way -- given a pivot vector \(\mathbf{v}\) and a parent vector \(\mathbf{x_p}\), we generate an opposite child \(\mathbf{x_c}\) as \(\mathbf{x_c} = \mathbf{x_p} + \mathbf{U}[(\frac{3d}{4}, \frac{5d}{4})] \circ (\frac{1}{d}(\mathbf{x_p} - \mathbf{v}))\). Here, \(d = ||\mathbf{v} - \mathbf{x_p}||\) and \(\mathbf{U}[(d,u)]\) is a uniform random vector where each element is within the range \([d,u]\). The overall procedure is presented in Algorithm \ref{algo:onsga2} in line 9--12 and illustrated in the Figure \ref{fig:opposite-creation}. The lines 9--12 in Algorithm \ref{algo:onsga2} can be recapped as follows: \(\mathbf{v}\) is on the true PF extreme and \(\mathbf{x_i}\) is \textit{far} from \(\mathbf{v}\), therefore, move \(\mathbf{x_i}\) closer to \(\mathbf{v}\) -- \textit{opposite} of \textit{far} is \textit{close}. Similar interpretation can be made when the vector \(\mathbf{v}\) is an intermediary \textit{gap}. 
%
\begin{algorithm}[tp]
\caption{NSGA-II with Opposition}
\label{algo:onsga2}
{\footnotesize
\begin{algorithmic}[1]
	\REQUIRE true PF extreme points $E^{\ast}$ from Algorithm \ref{algo:find-extreme-points}
	\STATE $N \leftarrow$ population size $|P_t|$
	\STATE $N_{\text{gen}} \leftarrow $ maximum generation
	\STATE $t \leftarrow 1$
	\WHILE{$t \le N_{\text{gen}}$}
		\STATE $P'_t \leftarrow$ select front-wise best 25\% solutions from $P_t$ and shuffle
		\STATE $E'_t \leftarrow$ construct pivot set $E'$ using algorithm \ref{algo:generate-pivot-points}
		\STATE $O_t \leftarrow \emptyset$
		\FOR{each solution $\mathbf{x_i} \in P'_t$}
			\STATE $S \leftarrow$ pick $k$ random solutions from $E'_t$
			\STATE $\mathbf{v} \leftarrow$ $\mathbf{v} \in S$ such that $\mathbf{v}$ is the furthest point from $\mathbf{x_i}$
			\STATE $d \leftarrow ||\mathbf{v} - \mathbf{x_i}||$
			\STATE $\mathbf{x_c} \leftarrow \mathbf{x_i} + \mathbf{U}[(\frac{3d}{4}, \frac{5d}{4})] \circ (\frac{1}{d}(\mathbf{x_i} - \mathbf{v}))$
			\STATE $x_j \leftarrow v_j \in \mathbf{v}$ if $x_j \in \mathbf{x_c} > x_{j_H}$ or $x_j \in \mathbf{x_c} < x_{j_L}$ 
			\STATE $O_t \leftarrow \{O_t \cup \mathbf{x_c}\}$
		\ENDFOR
		\STATE $P_t \leftarrow \{P_t \cup  E^\ast\}$
		\STATE $R_t \leftarrow \{P_t \cup Q_t\}$
		\STATE rank $R_t$ into fronts, $R_t \rightarrow \{\mathcal{F}_1, \mathcal{F}_2, \ldots, \mathcal{F}_n\}$
		\STATE $P_{t+1} \leftarrow \emptyset$
		\STATE $i \leftarrow 1$
		\WHILE{$|P_{t+1}| + |\mathcal{F}_i| \le N$}
			\STATE assign crowding distances on the front $\mathcal{F}_i$
			\STATE $P_{t+1} \leftarrow \{P_t \cup \mathcal{F}_i\}$
			\STATE $i \leftarrow i + 1$
		\ENDWHILE
		\STATE sort $\mathcal{F}_i$ in descending order using $\prec_n$
		\STATE $P_{t+1} \leftarrow$ the first $N - |P_{t+1}|$ solutions from $\mathcal{F}_i$
		\STATE $Q_{t+1} \leftarrow$ select, crossover and mutate $P_{t+1}$
		\STATE randomly insert all $x_i \in O_t$ into $Q_{t+1}$
		\STATE $t \leftarrow t + 1$
	\ENDWHILE
\end{algorithmic}}
\end{algorithm}
%
%
\begin{figure*}[bp!]
	\centering
	\subfloat[Convergence test for ZDT2 problem\label{subplot:zdt2-onsga2r}]{%
		\includegraphics[width=0.5\textwidth]{zdt2-onsga2r-nsga2r-hvstat}}
	\subfloat[Convergence test for ZDT3 problem\label{subplot:zdt3-onsga2r}]{%
		\includegraphics[width=0.5\textwidth]{zdt3-onsga2r-nsga2r-hvstat}}
	\hfill
	\subfloat[Convergence test for ZDT4 problem\label{subplot:zdt4-onsga2r}]{%
		\includegraphics[width=0.5\textwidth]{zdt4-onsga2r-nsga2r-hvstat}}
	\subfloat[Convergence test for ZDT6 problem\label{subplot:zdt6-onsga2r}]{%
		\includegraphics[width=0.5\textwidth]{zdt6-onsga2r-nsga2r-hvstat}}
	\caption{These plots illustrates the comparative analysis of the convergence rates for different 2-objective problems, the curves are actually consisted of box-plots. Here onsga2r denotes our algorithm and nsga2r is NSGA-II.}
	\label{plot:onsga2r-hv-zdt}
\end{figure*}
%

Moreover, upon generating the vector \(\mathbf{x_c}\), it may happen that one of the variable values go beyond the variable bounds (i.e. \(x_j > x_{j_H}\) or \(x_j < x_{j_L}\)), in that case, we replace the overshot value from the corresponding variable value \(v_j\) from the pivot point \(\mathbf{v}\). Therefore, if a certain vector \(\mathbf{x_c}\) can't make a successful translation, then \(\mathbf{x_c}\) is reverted back to the vicinity of \(\mathbf{v}\). Thus, we assure a local best estimated translation of the parent vector \(\mathbf{x_p}\). This process is done on the line 13 of the Algorithm \ref{algo:onsga2}.

When we apply this algorithm to NSGA-II, we follow the obvious way, the generated opposite population will be inserted into the child population \(Q_t\), the Algorithm \ref{algo:onsga2} also shows how to integrate everything in NSGA-II. Moreover, this algorithm is ``pluggable'' in a sense that we can integrate it to any other elitist EMO algorithm. In the following section, we are going to see in details, how our opposite generation algorithm drastically improves the convergence rate.

% Figure
\begin{figure}[tp!]
	\centering
	\includegraphics[width=0.5\textwidth]{zdt1-onsga2r-nsga2r-hvstat}
	\caption{The convergence test of Algorithm \ref{algo:onsga2} (onsga2r) vs. NSGA-II on problem ZDT1}
	\label{plot:zdt1-onsga2r}
\end{figure}
%
%
\begin{figure*}[tp!]
	\centering
	\subfloat[Exploration at generation 3 -- ZDT3 problem\label{subfig:zdt3-gen-3}]{%
		\includegraphics[width=0.5\textwidth]{zdt3-gen-3}}
	\subfloat[Exploration at generation 18 -- ZDT3 problem\label{subfig:zdt3-gen-18}]{%
		\includegraphics[width=0.5\textwidth]{zdt3-gen-18}}
	\hfill
	\subfloat[Exploration at generation 27 -- ZDT3 problem\label{subfig:zdt3-gen-27}]{%
		\includegraphics[width=0.5\textwidth]{zdt3-gen-27}}
	\subfloat[Exploration at generation 21 -- ZDT4 problem\label{subfig:zdt4-gen-21}]{%
		\includegraphics[width=0.5\textwidth]{zdt4-gen-21}}
		\caption{This figure illustrates how our algorithm deterministically identifies which front needs to be explored first and gradually discovers the entire PF. The example here demonstrates a 3 cases of ZDT3 problem (Figure \ref{subfig:zdt3-gen-3}--\ref{subfig:zdt3-gen-27}). The outlier dots represents the deterministically generated solutions that did not survived because they are weakly dominated. The darks dots are those that are deterministically generated and survived. In the case of ZDT4 (Figure \ref{subfig:zdt4-gen-21}), we can see how the bias and gaps have been corrected by our approach.}
	\label{fig:zdt3-gap}
\end{figure*}
%
\section{Experiments with the Benchmark Problem Set}
\label{sec:onsga2r-zdt}
First we have tested the performance of Algorithm \ref{algo:onsga2} on \(5\) \(2\)-objective problems \cite{zdt-set}, namely ZDT1, ZDT2, ZDT3, ZDT4 and ZDT6, and we have set NSGA-II as the control. To maintain a fair comparison, we have compensated the extra function evaluations by the Algorithm \ref{algo:find-extreme-points} for the NSGA-II runs, and compared NSGA-II and Algorithm \ref{algo:onsga2} side by side. The performance measure for our test was Hypervolume (HV) \cite{wfg}, and we are interested to see which algorithm can reach to a desired HV within less function evaluations (FE). For all problems, we have seen our algorithm can demonstrate a very steep convergence to the true PF, even when the extra FE from Algorithm \ref{algo:find-extreme-points} are compensated for NSGA-II. 

All the results are collected from 31 independent runs started with non-identical random seeds. In all plots, \textit{onsga2r} stands for Algorithm \ref{algo:onsga2}. The extra cost to find the extreme points are indicated with a ``T'' arrow on the x-axis. During computation of the HV measure, we have set the reference point to \(\{2.0,2.0\}\) for all problems except ZDT6, where it has been set to \(\{4.0, 4.0\}\)\footnote{The code that we have used to compute HV measure was taken from \url{http://www.wfg.csse.uwa.edu.au/hypervolume/index.html#code}, where the implementation assumes that all the objective values need to be on the one side of the reference point. For ZDT6, a closer reference point made the curves in the plots to be showing up very late at the end of x-axis.}.

The experiment with ZDT1 is illustrated in Figure \ref{plot:zdt1-onsga2r}, here we can see that the Algorithm \ref{algo:find-extreme-points} takes up to around 2K of function evaluations. Given that, NSGA-II still lags behind with a multiple factors to reach the desired PF. We have seen similar effect on all the rest of the problems ZDT2, ZDT3, ZDT4 and ZDT6. Except for ZDT3, we can see some fluctuations due the disconnected nature of the true PF. All the plots for the rest of the problems are presented in Figure \ref{plot:onsga2r-hv-zdt}.

There is another interesting observation we have made, once this opposite point generation scheme is used, the search process becomes more focused and works in a more predictive manner. As an example, in the case of ZDT3 problem (where the true PF consists of 5 disconnected curves), the algorithm first tries to fill up the first partition and gradually moves to the next. The algorithm automatically detects which portion of the PF needs to be addressed first and try to fill the gaps by deliberately injecting points to the vicinity of those gaps. As a result our model can infer which objective is hard to solve and deterministically decides which one needs to be explored more. This scenario is illustrated in Figure \ref{fig:zdt3-gap}, where we can see how the point generation algorithm moves from one disconnected front to the next.

Moreover, our approach can also efficiently solve the issue of \textit{search trajectory bias}, if we look at the Figure \ref{subfig:zdt4-gen-21}, we can see that the new solutions are deterministically generated where the explorations are not done thoroughly yet.

%
\begin{figure*}[tp!]
	\centering
	\subfloat[Convergence test for DTLZ1 problem\label{subplot:dtlz1-onsga2r}]{%
		\includegraphics[width=0.48\textwidth]{dtlz1-onsga2r-nsga2r-hvstat}}
	\subfloat[Convergence test for DTLZ2 problem\label{subplot:dtlz2-onsga2r}]{%
		\includegraphics[width=0.48\textwidth]{dtlz2-onsga2r-nsga2r-hvstat}}
	\hfill
	\subfloat[Convergence test for DTLZ3 problem\label{subplot:dtlz3-onsga2r}]{%
		\includegraphics[width=0.48\textwidth]{dtlz3-onsga2r-nsga2r-hvstat}}
	\subfloat[Convergence test for DTLZ5 problem\label{subplot:dtlz5-onsga2r}]{%
		\includegraphics[width=0.48\textwidth]{dtlz5-onsga2r-nsga2r-hvstat}}
	\hfill
	\subfloat[Convergence test for DTLZ6 problem\label{subplot:dtlz6-onsga2r}]{%
		\includegraphics[width=0.48\textwidth]{dtlz6-onsga2r-nsga2r-hvstat}}
	\subfloat[Convergence test for DTLZ7 problem, our algorithm shows better performance in terms of mean and max hypervolume.\label{subplot:dtlz6-onsga2r}]{%
		\includegraphics[width=0.48\textwidth]{dtlz7-onsga2r-nsga2r-hvstat}}
	\caption{These plots illustrates the comparative analysis of the convergence rates for different 3-objective problems, the curves are actually consisted of box-plots. Here onsga2r denotes our algorithm and nsga2r is NSGA-II.}
	\label{plot:onsga2r-hv-dtlz}
\end{figure*}
%
\subsection{Experiments with the Scalable Problem Set}
\label{sec:onsga2r-dtlz}
In the next experiment, we have carried out the similar tests with the scalable problem sets -- DTLZ1, DTLZ2, DTLZ3, DTLZ4, DTLZ5, DTLZ6 and DTLZ7 \cite{dtlz-set}. For all cases we have considered \(3\)-objectives. The control was NSGA-II results and similarly we compensate the measure with the extra FE to find extremes. All the results are collated from 31 independent runs. The reference point for HV computation has been set to \(\{2.0, 2.0, 2.0\}\) for DTLZ2, DTLZ4 and DTLZ5. For DTLZ1 and DTLZ7 it has been set to \(\{10.0, 10.0, 10.0\}\), for DTLZ3 it was \(\{15.0, 15.0, 15.0\}\) and for DTLZ6, it was \(\{4.0, 4.0, 4.0\}\). All the convergence plots are presented in the Figure \ref{plot:onsga2r-hv-dtlz}. 

Here we can see, in DTLZ6 our approach shows noticeable improvement, and for DTLZ1, DTLZ3 and DTLZ7 the opposite point generation offers even greater improvement. However, for DTLZ2, DTLZ4 and DTLZ5 the opposition scheme does not offer any improvement. What we have seen for these problems, NSGA-II does not face much difficulties to reach to the true PF therefore the outcome stays same even if we introduce extreme points to guide the search. For example DTLZ6 is harder than DTLZ5\footnote{DTLZ5 and DTLZ6 are basically the same except an exponential growth added to the \(g\) function, and DTLZ7 has a disconnected PF.}, as a result, our approach shows even better efficacy in solving harder problems. There is another interesting fact that we need to acknowledge -- there is no way that our approach will degrade the performance of the host algorithm (i.e. NSGA-II), since all the points we generate are no worse than the existing solutions in the population. The opposition scheme invariably adds improvements on the convergence if there is any. 

%
\begin{figure*}[pb!]
	\centering
	\subfloat[ZDT4 problem\label{subplot:zdt4-nsga2re}]{%
		\includegraphics[width=0.49\textwidth]{zdt4-onsga2r-nsga2re-hvstat}}
	\subfloat[DTLZ3 problem\label{subplot:dtlz3-nsga2re}]{%
		\includegraphics[width=0.49\textwidth]{dtlz3-onsga2r-nsga2re-hvstat}}
	\hfill
	\subfloat[DTLZ6 problem\label{subplot:dtlz6-nsga2re}]{%
		\includegraphics[width=0.49\textwidth]{dtlz6-onsga2r-nsga2re-hvstat}}
	\subfloat[DTLZ7 problem\label{subplot:dtlz6-nsga2re}]{%
		\includegraphics[width=0.49\textwidth]{dtlz7-onsga2r-nsga2re-hvstat}}
		\caption{These plots illustrates the comparative analysis of the convergence rates for different 2 and 3-objective problems, the curves are actually consisted of box-plots. Here onsga2r denotes our algorithm and nsga2re is the NSGA-II equipped with extreme points.}
	\label{plot:nsga2re-hv}
\end{figure*}
%
% Figure
\begin{figure}[!tp]
	\centering
	\includegraphics[width=0.5\textwidth]{dtlz4-longvar}
	\caption{The experiment with the varying variable length for DTLZ4 problem. `onsga2r' stands for our approach and `nsga2r' is the NSGA-II algorithm, \(n\) is the variable size. NSGA-II is also compensated for the extra function evaluations to find the true PF extremes. The lines for our algorithm shows the \textit{minimum} hypervolume achieved in each run; and for NSGA-II, we have taken the \textit{maximum} of it.}
	\label{plot:longvar}\vspace{-10.0pt}
\end{figure}
%
\subsection{Expanding the Search Space} 
\label{subsec:longvar}
For DTLZ2, DTLZ4 and DTLZ5 problems, we have already seen that the convergence gain was not noticeable. However, still we are not clear how our model will behave if the problem difficulty is increased. To investigate, we have increased the number of variables \(n\) to see if we can improve. For example we have picked one problem like DTLZ4 and did the same experiments with varying number of design variables. The standard setting for DTLZ4 is \(n = 12\), we have changed this length from \(12\) to \(60\) and \(96\). The outcome of this test is quite interesting -- the applicability of our approach is increased with the number of variables, and this effect is identical for DTLZ2 and DTLZ5 as well. This result for DTLZ4 is presented in the Figure \ref{plot:longvar}, here we can see at \(n = 96\), our approach shows the most gain in the convergence speed-up, and if we make \(n\) even bigger, the trend becomes more conspicuous. Moreover, in Figure \ref{plot:longvar}, we have shown \textit{minimum} hypervolume achieved with our approach and the \textit{maximum} possible hypervolume achieved by the NSGA-II algorithm.

For this experiment, we did not use MADS algorithm to find the extreme solutions, although these problems have local optima. The MADS algorithm is good for finding more accurate extreme points for small problems with local optima -- with smaller function evaluations, however, the performance of MADS degrades as the number of variables gets bigger, where a budgeted evaluation is a requirement. Our empirical investigation shows that for large variable space, IP methods give reasonably close (at least that are good for our purpose) extreme points within the acceptable function evaluations.

%
\begin{figure*}[!htp]
	\centering
	\subfloat[ZDT4 problem\label{subplot:zdt4-weak}]{%
		\includegraphics[width=0.49\textwidth]{zdt4-weak}}
	\subfloat[DTLZ1 problem\label{subplot:dtlz1-weak}]{%
		\includegraphics[width=0.49\textwidth]{dtlz1-weak}}
	\hfill
	\caption{These plots illustrates the comparative analysis of the convergence rates for 2 and 3-objective problems, the curves are actually consisted of box-plots. Here onsga2r denotes our algorithm and nsga2r is the NSGA-II. Here the Algorithm \ref{algo:onsga2} starts with deliberately injected weakly dominated extreme solutions. For ZDT4 it has been set to \(\{(1.0, 41.0), (0.0, 68.0)\}\) and for DTLZ1 it was \(\{(0.0, 217.5, 13.7),(67.6, 0.48, 0.0),(0.0, 0.0, 32.0)\}\).}
	\label{plot:weak-hv}
\end{figure*}
%
\subsection{Next Experiment: NSGA-II Equipped with Extreme Points}
\label{subsec:nsga2re}
We were also very curious to see, what if we introduce the extreme points to the NSGA-II so that it can utilize them to converge to the true PF. In this case, we compute the pivot points from the initial population, inject them and let the NSGA-II run. Here NSGA-II does not employ any opposite point generation scheme. All the test parameters were kept same as before and the results are summarized in the Figure \ref{plot:nsga2re-hv}. Here, we can see that the convergence rate is still noticeably improved, but NSGA-II with extreme points shows a noticeable variance during the convergence and our approach is more robust (i.e. shows less variance). This is because our approach generate new points in a very deterministic and predictable manner and the NSGA-II tries to emulate this by applying mutation/crossover. We also need to remember that we set aside only 25\% of the original population to be subjected to opposition. We only add a subset of our original results in the Figure \ref{plot:nsga2re-hv}, since all of them shows the similar trend as in Figure \ref{plot:onsga2r-hv-zdt} and \ref{plot:onsga2r-hv-dtlz}.

In the next sections we are going to experiments with more interesting what-if analysis -- how does our model perform what if the true PF extremes are not found, how does the deterministically generated points (i.e. opposite solutions) contribute to the search mechanism etc.

\subsection{Experiment with The Weakly Dominated Extreme Points}
\label{sec:weak-extremes}
In this paper, all the experiments were carried out on the standard benchmark problems popular among the EMO practitioners, however in other situation, it may be very hard to find the true PF extremes. In this experiment we will try to address this issue by deliberately setting the extreme points as weakly dominated solutions of varying distances from the true PF extremes. More specifically, for ZDT1 problem, the true PF extremes are located at \(\{(1.0, 0.0, (0.0, 1.0)\}\), but we will start the algorithm with \(\{(2.0, 0.0), (0.0, 2.0)\}\), \(\{(4.0, 0.0), (0.0, 4.0)\}\) etc. and see if we can still keep the performance (or if degrades, how do they degrade).

The results are shown for problems ZDT4 and DTLZ1 in Figure \ref{plot:weak-hv}, here we can see that for ZDT4 problem our approach runs with more variance however, the mean/median hypervolume values are still better than the NSGA-II runs. For DTLZ1, interestingly our approach shows steep rise in the convergence rate. However, in both cases, the cost of finding the weakly extreme points is not compensated, therefore both algorithms start with the same point on the x-axis. Even if such compensation were made, still our approach is better off than the NSGA-II for DTLZ1, but in the case for ZDT4, the trends will going to  be more overlapped. However, if we consider the mean/median HV measure, our approach still offers a better convergence speed and the similar trend has been noticed for all other problems. This experiment confirms that our approach is still viable even if the extreme points are \textit{very} far from the true PF-extremes.

%
\begin{figure*}[!hbp]
	\centering
	\subfloat[ZDT4 problem\label{subplot:zdt4-hv-trend}]{%
		\includegraphics[width=0.49\textwidth]{zdt4-hv-trend}}
	\subfloat[DTLZ3 problem\label{subplot:dtlz3-hv-trend}]{%
		\includegraphics[width=0.49\textwidth]{dtlz3-hv-trend}}
	\hfill
	\caption{These plots illustrates the comparative analysis of the mean-HV convergence rates for 2 and 3-objective problems, with varying rates for opposite point allocation ratio. For DTLZ3, we can see that the allocation ratio of \(90\%\) makes the most negative effect on the convergence rate.}
	\label{plot:hv-trends}
\end{figure*}
%
\section{Analysis of The Opposite Solution Survival Rates}
\label{sec:survival}
In the next part, we will see how the deterministically generated solutions survives the selection mechanism at the line 28 in Algorithm \ref{algo:onsga2}. To do this we assign flags to the generated opposite solutions in every generation and count how many of them are passing through the selection phase. Interestingly, in all of the cases, this survival rate is very high during the initial generations and steeply settles down to \(\sim30\%\). This was really confounding to notice why this rate almost always settles down to a specific number, especially in those cases when the opposition approach shows a better convergence pay-offs. This scenario is presented in the Figure \ref{plot:zdt1-survival}, where the highest survival rate (\(88\%\)) has been achieved at generation \(2\) and reached the equilibrium at \(33\%\). In order to make this illustrations more compact, we present these statistics in the Table \ref{table:survival}.

%
\begin{table*}[tp!]
	\caption{Mean Static Survival Rates for Different Problems}
	\label{table:survival}
	\centering
	{\renewcommand{\arraystretch}{1.5}
	\begin{tabular}{l|llllllll|llll}
	Problems		& ZDT1 & ZDT2 & ZDT4 & ZDT6 & DTLZ1 & DTLZ3 & DTLZ6 & DTLZ7 & ZDT3 & DTLZ2 & DTLZ4 & DTLZ5\\ \hline
	$\textit{Speed-up}(A,B)$	& \textbf{2.67} & \textbf{3.56} & \textbf{2.93} & \textbf{6.82} & \textbf{2.14} & \textbf{2.59} & \textbf{1.22} & \textbf{2.47} & 0.0 & 0.0 & 0.0 & 0.0\\ \hline
	Static Survival Rate (\(\%\))	& \textbf{33.0} & \textbf{32.6}	& \textbf{26.0} & \textbf{28.5} & \textbf{27.4} & 22.2 & 11.8 & \textbf{28.8} & 19.1 & 24.7 & 19.0 & \textbf{29.6}\\ \hline
	\end{tabular}}
\end{table*}
%
% Figure
\begin{figure}[tp]
	\centering
	\includegraphics[width=0.5\textwidth]{zdt1-onsga2r-survival}
	\caption{The survival rate problem ZDT1 over the generations, we can see the highest peek at generation \(2\) and settles down to \(33\%\) around generation \(60\).}
	\label{plot:zdt1-survival}
\end{figure}
%
Please also note that all the experiment data were collated from \(31\) independent runs, therefore the survival rate is calculated as the mean of those \(31\) runs at each generation. For a better quantification of this statistics, we have measured the \textit{Speed-up} ratio of a pair algorithms (Algorithm-\(A\), algorithm-\(B\)) in the most obvious way --
%
\begin{equation}
	\textit{Speed-up}(A,B) = \frac{\text{FE}_A \ge r \overline{\text{HV}}_{B_{\text{max}}}}{\text{FE}_B = \overline{\text{HV}}_{B_{\text{max}}}}
	\label{eq:speed-up}
\end{equation}
%
Here, \(\text{FE}_A\) and \(\text{FE}_B\) denotes the function evaluation for Algorithm-\(A\) and \(B\) respectively. \(\overline{\text{HV}}_{B_{\text{max}}}\) denotes the maxmium mean-HV measure for the Algorithm-\(B\) in the convergence plot; and \(r\) is a value \(0.0 < r \le 1.0\). Hence, the above expression computes the ratio of the function evaluation of \(A\) to reach \textit{at-least} a certain \(r\)-portion of the maximum of the mean-hypervolumes of \(B\) and the function evaluation of \(B\) to reach its maximum of the mean-hypervolumes. 

Therefore, if the Algorithm-\(A\) has a slower convergence rate than that of Algorithm-\(B\), then \(\textit{Speed-up}(A,B) > 0.0\), otherwise it will be equal to 0.0. For all the cases, we have set the value of \(r\) within \(0.8 \le r \le 0.9\) -- every time we chose the value of \(r\) depending on how much accurate we want to capture the speed-ups from the convergence plots. For example, in the Table \ref{table:survival}, the Algorithm-\(A\) is NSGA-II and \(B\) is our approach. Any \textit{Speed-up} value greater than 1.0 means our approach has a better convergence rate than that of NSGA-II.

%
\begin{figure*}[!htp]
	\centering
	\subfloat[ZDT4 problem\label{subplot:zdt4-survival-trend}]{%
		\includegraphics[width=0.49\textwidth]{zdt4-survival-trend}}
	\subfloat[DTLZ3 problem\label{subplot:dtlz3-survival-trend}]{%
		\includegraphics[width=0.49\textwidth]{dtlz3-survival-trend}}
	\hfill
	\caption{These plots illustrates the mean-static survival-rates for 2 and 3-objective problems, with varying rates for opposite point allocation ratio. In both cases we can see that the rate of survival decreases with increase in the allocation ratio.}
	\label{plot:survival-trends}
\end{figure*}
%
%
\begin{figure*}[!hbp]
	\centering
	\subfloat[at generation 6, hitting the true PF\label{subplot:zdt4-hns-gen-6}]{%
		\includegraphics[width=0.34\textwidth]{zdt4-hns-gen-6}}
	\subfloat[at generation 15, spreading upward\label{subplot:zdt4-hns-gen-15}]{%
		\includegraphics[width=0.34\textwidth]{zdt4-hns-gen-15}}
	\subfloat[at generation 36, spreading downward\label{subplot:zdt4-hns-gen-36}]{%
		\includegraphics[width=0.34\textwidth]{zdt4-hns-gen-36}}
	\hfill
	\caption{This figure illustrates the ``hit-and-spread'' nature of the single pivot point case. This plot demonstrated a scenario for ZDT4 problem, here we can clearly see trajectory of the optimization procedure.}
	\label{plot:hit-n-spread}
\end{figure*}
%
\subsection{Varying The Opposite Solution Allocation Ratio}
\label{subsec:op-ratio}
However, the above statistics neither says much about why there have always been a static ratio of survival rates nor it will be a constant if we change the number of allocated solution for opposition. In all of our experiments, this allocation has been set to \(25\%\), therefore, if the population size is \(100\) and if the survival rate is \(30\%\), then around only \(25 \times 0.3 = 7.5 \sim 8\) solutions help to achieve such convergence gain. Now we want to see, if it is possible to improve the convergence by increasing the opposite solution allocation ratio from \(25\%\). However, it turns out be a discouraging attempt, when we increase this allocation rate, the survival rate also decreases accordingly. Even for some problems, a smaller allocation ratio has been found to be more effective. This statistics has been presented in Figure \ref{plot:hv-trends} and \ref{plot:survival-trends}, where for each cases, the allocation ratio has been changed from \(10\%\) to \(90\%\).

Interestingly, for DTLZ3 if the opposite solution allocation ratio is \(10\%\), it shows the best \textit{Speed-up}, then it degrades as the allocation ratio has been increased. But for ZDT4, the variance is not much noticeable. However, in general, we have seen that a smaller opposite allocation ratio tends to show better convergence speed.\vfill \eject

%
\begin{figure*}[!htp]
	\centering
	\subfloat[DTLZ3 problem\label{subplot:dtlz3-single-pf}]{%
		\includegraphics[width=0.49\textwidth]{dtlz3-onsga2r-onsga2rw-hvstat}}
	\subfloat[DTLZ6 problem\label{subplot:dtlz6-single-pf}]{%
		\includegraphics[width=0.49\textwidth]{dtlz6-onsga2r-onsga2rw-hvstat}}
	\hfill
	\caption{Similar convergence plots for the algorithm with single intermediary pivot point. Here we can see a noticeable improvement on the DTLZ3 problem.}
	\label{plot:single-pf}
\end{figure*}
%
%
\begin{figure*}[!htb]
	\centering
	\subfloat[ZDT4 problem -- the \textit{inverse-opposite} is slightly better\label{subplot:zdt4-hv-opschemes}]{%
		\includegraphics[width=0.49\textwidth]{zdt4-hv-opschemes}}
	\subfloat[ZDT6 problem -- the \textit{random} is slightly better\label{subplot:zdt6-hv-opschemes}]{%
		\includegraphics[width=0.49\textwidth]{zdt6-hv-opschemes}}
	\hfill
	\caption{The mean-HV convergence plots for the algorithm with inverse notion of \textit{opposition} used in the original algorithm. Here we can see there is not much change in the convergence rates. The black line is from the mean-HV values of the original \textit{opposition} based algorithm, the grey line is found from the \textit{inverse-opposition} based pivot selection scheme and the light line corresponds to a \textit{random} pivot selection scheme.}
	\label{plot:opschemes}
\end{figure*}
%
\section{Applying Opposition Over A Single True-PF Solution}
\label{sec:single-pf}
So far, our focus was to create the complete PF from limited information -- namely using \(k\)-number of true-PF extremes. However, this approach also comes with some limitations. For example, in many hard problems, where there are lots of local optima, it is sometimes hard to exactly pin-point the true-PF extremes. For 3-objective problems, it is also sometimes hard to find all the true-PF extremes and we end up locating only a subset of them or some weakly dominated extreme points. Moreover, as we are allocating a fixed budget for finding the true extremes, the allocation of function evaluation for each objective decreases as the number of objectives become larger. Therefore, this approach may not be much useful if we want to solve a \textit{many-objective} problem.

In such cases, finding \(k\)-number of true-PF extremes might not be a practical way, therefore we need to come-up with a more streamlined approach to tackle this problem. In this experiment, we will try to see what if we provide only \textit{one} intermediary (non-extreme) true-PF solution to our algorithm. In this case, we are concerned with finding only \textit{one} (and \textit{any}) point on the true-PF, thus drastically reducing the computational cost associated with gleaning the ``limited'' information. To do this, we follow a similar (yet even more simple) approach as presented in the Algorithm \ref{algo:find-extreme-points}. In this case, in stead of considering each objective separately, we will solve a weighted sum of all the objectives -- during the first \(\frac{T}{2}\)-budget, we will optimize the function in the form of \(f_w(\mathbf{x}) = \sum_{j=1}^k w_j f_j(\mathbf{x})\), where \(w_j = \frac{1}{k}\). Obviously, for the AASF part, we will use the same function except taking the weights as \(w_j = \frac{1}{k}\). Therefore, now the loop in the line 6 in Algorithm \ref{algo:find-extreme-points} can be disregarded and the total budget is now basically \(\frac{1}{k}\)-th of the previous cost.

Now, at the beginning, the set \(E^\ast\) in the Algorithm \ref{algo:generate-pivot-points} will contain only 1 point (instead of \(k\)) and the rest of the routines have been kept as same as before for both Algorithm \ref{algo:generate-pivot-points} and \ref{algo:onsga2}. In the Algorithm \ref{algo:onsga2}, the procedure will now start to optimize by accumulating solutions near one pivot point and gradually try to explore by finding new extremes from that point. In fact, now the algorithm behaves like in a ``hit-and-spread'' manner -- first it will try to hit the true-PF on the vicinity of the pivot solution and gradually, once it starts to getting the extremes, the algorithm will try to populate the set \(E_c\), \(G\) and \(E\) in the same manner. This mechanism is illustrated in the Figure \ref{plot:hit-n-spread}, where we can see how the algorithm first hits the pivot point on the true-PF and then it starts to spread left and right over the true Pareto-front.

Interestingly, this approach shows a noticeable improvement over the original algorithm for a certain problems -- especially in DTLZ3 and DTLZ6. For the rest of the problems we could not find much difference but slightly better in some other cases. The convergence plots for this approach on the problem DTLZ3 and DTLZ6 are presented in the Figure \ref{plot:single-pf}. Here we can see, for DTLZ3 the improvement is even better. However, there is one limitation that we need to acknowledge for this approach, if the true-PF is disconnected then it will be very difficult to locate an intermediary pivot point since there is no way to infer the \(w_j\) values that will ensure a valid point on the true-PF, the same argument applied when the problem is constrained.

\section{Disregarding The Notion of Opposition}
\label{sec:inv-opp}
The central part of our study is the meaningful integration of the notion of \textit{opposition} in EMO algorithms. Therefore, we have adopted the semantics of opposition as \textit{far-vs-close} and \textit{crowded-vs-isolated} etc., and to precisely implement it, we are choosing the pivot points \(\mathbf{v}\) that is the \textit{farthest} from current parent \(\mathbf{x_p}\). Now, one can ask, what if we do not do it in this way, what if we choose the \textit{closest} pivot from the parent instead of the \textit{farthest} pivot ? More specifically, our concern is if \textit{the notion of opposition} being really helpful or not.

To address this query, we have changed the line 10 of the Algorithm \ref{algo:onsga2} to pick the \textit{closest} point \(\mathbf{v}\) instead of the \textit{farthest} one. We have also tested with a version where we pick the pivot point in random -- completely disregarding any distance measure from the parent solution. Such a setup now comes with a very interesting result -- it turns out that the semantics of \textit{far-vs-close} is not that important at all. However, for the purpose of the \text{gap}-filling, the property of \textit{crowded-vs-isolated} is important. This experiment is presented in the Figure \ref{plot:opschemes}, where the results are presented for ZDT4 and ZDT6 problems. We can see that whatever method we use to pick the pivot point (i.e. random or inverse-opposite), the overall convergence changes are very negligible. Because, whatever point \(\mathbf{v}\) we choose, if we can generate \textit{any} solution near it, it will always be useful (i.e. non-dominated). So, the gain of the convergence speed actually does not come from the state of being close (or far) from the true-PF -- wherever we are, if we can generate \textit{any} point near \textit{any} pivot, we can win the convergence race.

Interestingly, this observation once-again confirms the notion of the \textit{generality} in the trade-off solutions, upon which the concept of so-called ``innovization'' \cite{innovization} depends. The ``innovization'' principle deals with finding the most \textit{general} properties from the solutions on the true-PF, and such properties are again utilized to construct better designs. In this case, our solution generation scheme inadvertently uses this principle to generate non-dominated solutions. For example, in the case of ZDT4 problem, the total number of variable is 10; among them, the first variable dictates the trade-off on the Pareto-front, all the solutions on the true-PF has same variable values from variable 2 to 10 which is a solution to Rastrigin's problem. Therefore, if we can locate at least one solution on the true-PF, any solution on its vicinity will share the similar variable values that is the solution to Rastrigin's problem. By following the same argument, we can assume that if we perturb the pivot solution on the true-PF, we supposed to get other non-dominated solutions, however such technique has been already discussed in the sub-section \ref{subsec:nsga2re}, and we have seen that such approach does not work well. Therefore, our approach offers an informative and a deterministic way to utilize this \textit{Pareto-set generality} to generate other solutions on the true-PF.   

\section{Computation Cost}
\label{sec:big-o}
The principal overhead for our approach is the complexity incurred by the Algorithm \ref{algo:find-extreme-points}, where the computational cost comes from the IP and the MADS algorithm. However, this IP or MADS function call is a one-pass process. Therefore, we are going to see the how the rest of the algorithm imposes the extra computational overhead on the host algorithm (i.e. NSGA-II). The line 6 in Algorithm \ref{algo:onsga2} is a call to Algorithm \ref{algo:generate-pivot-points}. The expensive part of this algorithm is line 11 -- which is in fact, a linear function of \(N\) (\(N = \text{population size}\)), i.e. \(\Theta(N)\) and all the other terms are function of \(k\) (\(k = \text{no. of objectives}\)). Therefore the overall complexity for the Algorithm \ref{algo:generate-pivot-points} is \(O(N)\). The loop 8--14 takes exactly \(\Theta(\frac{N}{4}) < O(N)\), the same goes with line 29. Therefore, our approach does not add any extra computational cost to NSGA-II algorithm.

\section{Conclusions and Future Works}
\label{sec:conclusion}
The main contribution of this paper is the incorporation of opposite point generation scheme in a different perspective. We have shown that the notion of \textit{opposition} can be utilized in a different ways. Our approach also shows that a simple and a deterministic scheme can aid the EMO optimization algorithm in a very interesting way. Our technique is also easy to implement and offers less overhead to the host algorithm. This approach can also correct the search bias introduced by the problem difficulty in an automated and predictable manner. We have also seen that the efficacy of our algorithm becomes more salient with the increasing level of problem difficulty. Even though we have carried out a quite extensive study of our model on a variety of benchmark problems, but we did not do any study on real world problems. The original idea of the \textit{Opposition Based Learning (OBL)} is quite interesting; and we can make more of it if this idea is utilized in a more meaningful way -- we think this is the main contribution of our study.  

However, in the future we want to address some other interesting issues with our current study, especially in the ``many-objective'' problems. As we have seen, our approach spends a good portion of the function evaluations to compute the extreme points for each objective separately. This overhead becomes more of a problem when the number of objective is multiplied. In such case, our current model might not be much useful. Moreover, finding the extreme points itself comes with its own set of problems -- i)finding the accurate extreme points are not guaranteed ii)the identification of extremes becomes more problematic when the true PF is a disconnected front iii) notionally, the extreme points are harder to find than any other non-extreme points on the true PF as ``being extreme'' could be interpreted as another constraint. Therefore, the integral dependency on the extreme points -- we think a limitation of our approach.

In the future work, we are going to address these issues. Moreover, we think there are also a possible scope in changing the idea \textit{opposition} for reference point based many-objective algorithms like MOEA/D and NSGA-III \cite{nsga3-main-p1}\cite{nsga3-main-p2}. In the future research, we hope to investigate these ideas. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}

\subsection{Subsection Heading Here}
Subsection text here.

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

\subsubsection{Subsubsection Heading Here}
Subsubsection text here.


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.

\section{Conclusion}
The conclusion goes here.

% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


\appendices
\section{Proof of the First Zonklar Equation}
Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
\section{}
Appendix two text goes here.


% use section* for acknowledgment
\section*{Acknowledgment}


The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

\end{comment}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEabrv,report}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
% \begin{thebibliography}{1}
% 
% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
% 
% \end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% \begin{IEEEbiography}{Michael Shell}
% Biography text here.
% \end{IEEEbiography}

% if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{John Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


